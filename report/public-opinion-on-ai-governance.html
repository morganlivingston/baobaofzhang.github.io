<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Artificial Intelligence: American Attitudes and Trends</title>
  <meta name="description" content="Artificial Intelligence: American Attitudes and Trends">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Artificial Intelligence: American Attitudes and Trends" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Artificial Intelligence: American Attitudes and Trends" />
  
  
  

<meta name="author" content="Baobao Zhang and Allan Dafoe">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="general-attitudes-toward-ai.html">
<link rel="next" href="ai-policy-and-u-s-china-relations.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href=""><b>Table of contents</b></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="overview-ai-and-american-public-opinion.html"><a href="overview-ai-and-american-public-opinion.html"><i class="fa fa-check"></i>Overview: AI and American public opinion</a><ul>
<li class="chapter" data-level="" data-path="overview-ai-and-american-public-opinion.html"><a href="overview-ai-and-american-public-opinion.html#key-findings"><i class="fa fa-check"></i>Key findings</a></li>
<li class="chapter" data-level="" data-path="overview-ai-and-american-public-opinion.html"><a href="overview-ai-and-american-public-opinion.html#reading-notes"><i class="fa fa-check"></i>Reading notes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html"><i class="fa fa-check"></i>General attitudes toward AI</a><ul>
<li class="chapter" data-level="" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#more-americans-support-than-oppose-developing-ai"><i class="fa fa-check"></i>More Americans support than oppose developing AI</a></li>
<li class="chapter" data-level="" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#gender-education-income-and-experience-with-tech-predict-support-for-developing-ai"><i class="fa fa-check"></i>Gender, education, income, and experience with tech predict support for developing AI</a></li>
<li class="chapter" data-level="" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#an-overwhelming-majority-of-americans-think-that-ai-and-robots-should-be-carefully-managed"><i class="fa fa-check"></i>An overwhelming majority of Americans think that AI and robots should be carefully managed</a></li>
<li class="chapter" data-level="" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#harmful-consequences-of-ai-in-context-of-other-global-risks"><i class="fa fa-check"></i>Harmful consequences of AI in context of other global risks</a></li>
<li class="chapter" data-level="" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#americans-understanding-of-key-technology-terms"><i class="fa fa-check"></i>Americans’ understanding of key technology terms</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="public-opinion-on-ai-governance.html"><a href="public-opinion-on-ai-governance.html"><i class="fa fa-check"></i>Public opinion on AI governance</a><ul>
<li class="chapter" data-level="" data-path="public-opinion-on-ai-governance.html"><a href="public-opinion-on-ai-governance.html#among-ai-governance-challenges-americans-prioritize-data-privacy-and-preventing-ai-enhanced-cyber-attacks-surveillance-and-digital-manipulation."><i class="fa fa-check"></i>Among AI governance challenges, Americans prioritize data privacy and preventing AI-enhanced cyber attacks, surveillance, and digital manipulation.</a></li>
<li class="chapter" data-level="" data-path="public-opinion-on-ai-governance.html"><a href="public-opinion-on-ai-governance.html#less-concern-about-ai-governance-challenges-is-expressed-by-americans-who-are-younger-who-have-cs-or-engineering-degress"><i class="fa fa-check"></i>Less concern about AI governance challenges is expressed by Americans who are younger, who have CS or engineering degress</a></li>
<li class="chapter" data-level="" data-path="public-opinion-on-ai-governance.html"><a href="public-opinion-on-ai-governance.html#americans-place-the-most-trust-in-the-u.s.-military-and-universities-to-build-ai-trust-tech-companies-and-non-governmental-organizations-more-than-the-government-to-manage-the-technology"><i class="fa fa-check"></i>Americans place the most trust in the U.S. military and universities to build AI; trust tech companies and non-governmental organizations more than the government to manage the technology</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ai-policy-and-u-s-china-relations.html"><a href="ai-policy-and-u-s-china-relations.html"><i class="fa fa-check"></i>AI policy and U.S.-China relations</a><ul>
<li class="chapter" data-level="" data-path="ai-policy-and-u-s-china-relations.html"><a href="ai-policy-and-u-s-china-relations.html#americans-underestimate-the-u.s.-and-chinas-ai-research-and-development"><i class="fa fa-check"></i>Americans underestimate the U.S. and China’s AI research and development</a></li>
<li class="chapter" data-level="" data-path="ai-policy-and-u-s-china-relations.html"><a href="ai-policy-and-u-s-china-relations.html#communicating-the-dangers-of-a-u.s.-china-arms-race-requires-explaining-policy-trade-offs"><i class="fa fa-check"></i>Communicating the dangers of a U.S.-China arms race requires explaining policy trade-offs</a></li>
<li class="chapter" data-level="" data-path="ai-policy-and-u-s-china-relations.html"><a href="ai-policy-and-u-s-china-relations.html#americans-see-the-potential-for-u.s.-china-cooperation-on-some-ai-governance-challenges"><i class="fa fa-check"></i>Americans see the potential for U.S.-China cooperation on some AI governance challenges</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="trend-across-time-attitudes-toward-workplace-automation.html"><a href="trend-across-time-attitudes-toward-workplace-automation.html"><i class="fa fa-check"></i>Trend across time: attitudes toward workplace automation</a><ul>
<li class="chapter" data-level="" data-path="trend-across-time-attitudes-toward-workplace-automation.html"><a href="trend-across-time-attitudes-toward-workplace-automation.html#americans-do-not-think-that-labor-market-disruptions-will-increase-with-time"><i class="fa fa-check"></i>Americans do not think that labor market disruptions will increase with time</a></li>
<li class="chapter" data-level="" data-path="trend-across-time-attitudes-toward-workplace-automation.html"><a href="trend-across-time-attitudes-toward-workplace-automation.html#extending-the-historical-time-trend"><i class="fa fa-check"></i>Extending the historical time trend</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html"><i class="fa fa-check"></i>High-level machine intelligence</a><ul>
<li class="chapter" data-level="" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html#arrivesooner"><i class="fa fa-check"></i>The public think high-level machine intelligence will arrive sooner than AI experts predict</a></li>
<li class="chapter" data-level="" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html#americans-express-mixed-support-for-developing-high-level-machine-intelligence"><i class="fa fa-check"></i>Americans express mixed support for developing high-level machine intelligence</a></li>
<li class="chapter" data-level="" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html#men-partisans-high-income-americans-and-those-with-tech-experience-express-greater-support-for-high-level-machine-intelligence"><i class="fa fa-check"></i>Men, partisans, high-income Americans, and those with tech experience express greater support for high-level machine intelligence</a></li>
<li class="chapter" data-level="" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html#the-public-expects-high-level-machine-intelligence-to-be-more-harmful-than-good"><i class="fa fa-check"></i>The public expects high-level machine intelligence to be more harmful than good</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appmethod.html"><a href="appmethod.html"><i class="fa fa-check"></i>Appendix A: Methodology</a><ul>
<li class="chapter" data-level="" data-path="appmethod.html"><a href="appmethod.html#yougovsampling"><i class="fa fa-check"></i>YouGov sampling and weights</a></li>
<li class="chapter" data-level="" data-path="appmethod.html"><a href="appmethod.html#appdemosubgroups"><i class="fa fa-check"></i>Demographic subgroups</a></li>
<li class="chapter" data-level="" data-path="appmethod.html"><a href="appmethod.html#appanalysis"><i class="fa fa-check"></i>Analysis</a></li>
<li class="chapter" data-level="" data-path="appmethod.html"><a href="appmethod.html#data-sharing"><i class="fa fa-check"></i>Data sharing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html"><i class="fa fa-check"></i>Appendix B: Topline questionnaire</a><ul>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#global_risks"><i class="fa fa-check"></i>Global risks</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#considersai"><i class="fa fa-check"></i>Survey experiment: what the public considers AI, automation, machine learning, and robotics</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#knowledge-of-computer-science-cstechnology"><i class="fa fa-check"></i>Knowledge of computer science (CS)/technology</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#supportdevai"><i class="fa fa-check"></i>Support for developing AI</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#manageexp"><i class="fa fa-check"></i>Survey experiment: AI and/or robots should be carefully managed</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#trustdevai"><i class="fa fa-check"></i>Trust of actors to develop AI</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#trustmanageai"><i class="fa fa-check"></i>Trust of actors to manage AI</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#govchallenges"><i class="fa fa-check"></i>AI governance challenges</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#airesearchcompare"><i class="fa fa-check"></i>Survey experiment: comparing perceptions of U.S. vs. China AI research and development</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#armsraceexp"><i class="fa fa-check"></i>Survey experiment: U.S.-China arms race</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#control"><i class="fa fa-check"></i>Control</a><ul>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#nationalism-treatment"><i class="fa fa-check"></i>Nationalism treatment</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#war-risks-treatment"><i class="fa fa-check"></i>War risks treatment</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#common-humanity-treatment"><i class="fa fa-check"></i>Common humanity treatment</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#uschinacoop"><i class="fa fa-check"></i>Issue areas for possible U.S.-China cooperation</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#jobtime"><i class="fa fa-check"></i>Trend across time: job creation or job loss</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#forecasthlmi"><i class="fa fa-check"></i>High-level machine intelligence: forecasting timeline</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#supporthlmi"><i class="fa fa-check"></i>Support for developing high-level machine intelligence</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#expectedoutcome"><i class="fa fa-check"></i>Expected outcome of high-level machine intelligence</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html"><i class="fa fa-check"></i>Appendix C: Additional data analysis results</a><ul>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#aawhatsai"><i class="fa fa-check"></i>Survey experiment: what the public considers AI, automation, machine learning, and robotics</a><ul>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#addcarefullym"><i class="fa fa-check"></i>Survey experiment: AI and/or robots should be carefully managed</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#appgovchallenges"><i class="fa fa-check"></i>AI governance challenges: prioritizing governance challenges</a></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#trust-in-various-actors-to-develop-and-manage-ai-in-the-interest-of-the-public"><i class="fa fa-check"></i>Trust in various actors to develop and manage AI in the interest of the public</a></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#appuschinacomp"><i class="fa fa-check"></i>Survey experiment: comparing perceptions of U.S. vs. China AI research and development</a></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#survey-experiment-u.s.-china-arms-race"><i class="fa fa-check"></i>Survey experiment: U.S.-China arms race</a></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#appjobloss"><i class="fa fa-check"></i>Trend across time: job creation or job loss</a></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#appsupporthlmi"><i class="fa fa-check"></i>Support for developing high-level machine intelligence</a></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#expected-outcome-of-high-level-machine-intelligence"><i class="fa fa-check"></i>Expected outcome of high-level machine intelligence</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a><ul>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#primary-researchers"><i class="fa fa-check"></i>Primary researchers</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#editing-and-design"><i class="fa fa-check"></i>Editing and design</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#funders"><i class="fa fa-check"></i>Funders</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#for-media-or-other-inquiries"><i class="fa fa-check"></i>For media or other inquiries</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#recommended-citation"><i class="fa fa-check"></i>Recommended citation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html"><i class="fa fa-check"></i>About us</a><ul>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#about-the-governance-of-ai-program"><i class="fa fa-check"></i>About the Governance of AI Program</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#about-the-future-of-humanity-institute"><i class="fa fa-check"></i>About the Future of Humanity Institute</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://governance.ai">Governance of AI Program</a></li>
<li><a href="https://www.fhi.ox.ac.uk/">Future of Humanity Institute</a></li>
<li><a href="http://www.ox.ac.uk/">University of Oxford</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Artificial Intelligence: American Attitudes and Trends</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="public-opinion-on-ai-governance" class="section level1">
<h1>Public opinion on AI governance</h1>
<div id="among-ai-governance-challenges-americans-prioritize-data-privacy-and-preventing-ai-enhanced-cyber-attacks-surveillance-and-digital-manipulation." class="section level2">
<h2>Among AI governance challenges, Americans prioritize data privacy and preventing AI-enhanced cyber attacks, surveillance, and digital manipulation.</h2>
<!-- 
BZ: I think this section is solid -- maybe 95% there. The literature review section could be cut because I'm not sure it's 100% relevant to the results.
AD: "over solid"? What does that mean? 
[BZ: I made a typo]

AD: why do we call this "attitudes", vs "beliefs"? Just wondering. Seems like "beliefs" is more respectful.
[BZ: I changed it to "public opinion".]
-->
<p>We sought to understand how Americans prioritize policy issues associated with AI. Respondents were asked to consider five AI governance challenges, randomly selected from <a href="apptopline.html#govchallenges">13 potential ones</a>; the order of the governance challenges presented to respondents was also randomized.</p>
<!-- 
AD: did we present in random order? if so, should mention. If not, worth clarifying
[BZ: I included information saying the order is randomized. Also I changed the wording of the AI governance challenges to be consistent.]
-->
<p>After considering each governance challenge, respondents were asked how likely they think the challenge will affect large numbers of people 1) in the U.S. and 2) around the world within 10 years.</p>
<p>We use scatterplots to visualize our survey results. In Figure , the <em>x</em>-axis is the perceived likelihood of the problem happening to large numbers of people in the U.S. In Figure , the <em>x</em>-axis is the perceived likelihood of the problem happening to large numbers of people around the world. The <em>y</em>-axes of the two plots are the same. A dot represents the mean perceived likelihood and issue importance, and the correspondent ellipse represents the 95% confidence region.</p>
<p>Americans consider all the AI governance challenges we present to be important: the mean perceived issues importance of each governance challenge is between “somewhat important” (2) and “very important” (3), though there is meaningful and discernible variation across items.</p>
<!--
AD: I think you are misread the ifgures. In around the world, the max is higher by about 5%. I don't think teh variation is greater, so much as the range is shifted up. Anyhow, variance doesn't seem like an important point. If you think it is, can you include a statistic for it, which we're sure is different. 

While the variation in perceived issue importance is small, there are greater variations in Americans' perceived likelihood of each governance challenge impacting a large number of people in the U.S., versus in the world, in the next ten years. 


 -- ranging from 55% likelihood for lethal autonomous weapons to 69% likelihood for AI-enhanced surveillance in the U.S. 

[BZ: I made big changes to the interpretation after formally testing the perceived likelihoods and found that there are statistically significant differences. See Table 141 for the results of the formal tests.]
 -->
<p>The AI governance challenges Americans think are most likely to impact large numbers of people, and that are important for tech companies and governments to tackle, are found in the upper-right quadrant of the two plots. These issues include data privacy, as well as AI-enhanced cyber attacks, surveillance, and digital manipulation. We note that these issues have been widely reported in the media during the time of the survey.</p>
<p>There are a second set of governance challenges that are perceived on average, as about 7% less likely, and marginally less important. These include autonomous vehicles, value alignment, bias in using AI for hiring, the U.S.-China arms race, disease diagnosis, and technological unemployment. Finally, a third set of challenges are perceived on average another 5% less likely, and about equally important, which include criminal justice bias and critical AI systems failures.</p>
<p>We also note that Americans predict that all of the governance challenges mentioned in the survey, besides protecting data privacy and ensuring the safety of autonomous vehicles, are <a href="addresults.html#appgovchallenges">more likely to impact people around the world than to affect people in the U.S</a>. While most of the statistically significant differences are substantively small, one difference stands out: Americans think that autonomous weapons are 7.6 percentage points more likely to impact people around the world in the U.S.</p>
<!-- 
I'm confused about this last paragraph. It seems your summary doesn't correlate with what we found. "threat to humanity", and tech unemployment are not in the top right quadrant.
I'm just unclear what you are trying to say. 

Also, we should probably remark on what scores lower. See the text I added, including LIST THEM part.

We could potentially say more about each of these. However, I don't really have the energy for it right now, and it could open cans of worms. So I lean toward keeping it like this. 

[BZ: I decided to remove the following paragraph.]
The concerns that our respondents express are reflected in the media. For instance, the _New York Times_'s coverage of concerns about humans losing control of AI and AI behaving unethically have significantly increased in recent years [@fast2017long]. While the media narrative of AI remains mostly positive, the public is disproportionately engaged with articles about AI's threat to humanity, lack of AI progress, concerns about AI-enhanced surveillance, and technological unemployment on social media [@dhs2017].
-->
<div class="figure"><span id="fig:airisksus"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/airisksus-1.png" alt="Perceptions of AI governance challenges in the U.S." width="2100" />
<p class="caption">
Figure 10: Perceptions of AI governance challenges in the U.S.
</p>
</div>
<div class="figure"><span id="fig:airisksworld"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/airisksworld-1.png" alt="Perceptions of AI governance challenges around the world" width="2100" />
<p class="caption">
Figure 11: Perceptions of AI governance challenges around the world
</p>
</div>
</div>
<div id="less-concern-about-ai-governance-challenges-is-expressed-by-americans-who-are-younger-who-have-cs-or-engineering-degress" class="section level2">
<h2>Less concern about AI governance challenges is expressed by Americans who are younger, who have CS or engineering degress</h2>
<!--
BZ: I think this section is pretty solid. 

AD: Looking at Appendix C, figure "AI governance challenges: issue importance by U.S. demographic subgroups" (Can we label figures? Also, can we make the hyperlinks go directly to the right figure?), I'm confused. What are these numbers? They don't seem to be the absolute difference from the mean, since the first figure (the blue one, with the same title) varies by magnitudes of 10, and this second one by <0.3. Are these standardized differences? There's no caption explaining it.  
In the first figure: White seems 10 points higher than non-white. Less than 30k is about 5 points lower than >100k. 
[BZ: The outcomes are based on the 4-point scale of issue importance and not on the percentage numbers of the blue graph. I re-wrote the intro text in the appendix subsection to emphasize this difference. Hopefully it will cut down on the confusion.]

Can we use two colors for the first figure (eg dark red to dark blue, or like you did later dark green to dark purple)? I think easier to see that, then just white to dark blue.
[BZ: I am against this in principle because we are going along a continuous scale without a clear "middle" (like going through zero).]
 -->
<p>We performed further analysis by estimating the percentage of respondents in each subgroup who consider each governance challenge to be “very important” for governments and tech companies to manage. (See <a href="addresults.html#appgovchallenges">Appendix C</a> for additional data visualizations.) In general, differences in responses are more salient across demographic subgroups than across governance challenges. In a linear regression model predicting perceived issue importance using demographic subgroups, governance challenges, and the interaction between the two, we find that the stronger predictors are demographic subgroup variables, including age group and having CS or programming experience. Two highly visible patterns emerge from our data visualization. First, a higher percentage of older respondents, compared with younger respondents, consider nearly all AI governance challenges to be “very important.” As discussed previously, we find that older Americans, compared with younger Americans, are less supportive of developing AI. Our results here might explain this: older Americans see each AI governance challenge as substantially more important than do younger Americans. Whereas 85% of Americans older than 68 consider each of these issues to be very important, only 40% of Americans younger than 34 do.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
<!-- AD please check preceding paragraph, and  prob make numbers more principled. Perhaps for the last thing estimate a linear regression based on actual age? 

[BZ: Checked and see the footnote I have added above. I will need to add the regression table to the appendix.]
-->
<p>Second, those with CS or engineering degrees, compared with those who do not, rate all AI governance challenges as less important. This result could explain our previous finding that those with CS or engineering degrees tend to exhibit greater support for developing AI.</p>
<div class="figure"><span id="fig:airisksdemo"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/airisksdemo-1.png" alt="AI governance challenges: issue importance by demographic subgroups" width="2100" />
<p class="caption">
Figure 12: AI governance challenges: issue importance by demographic subgroups
</p>
</div>
</div>
<div id="americans-place-the-most-trust-in-the-u.s.-military-and-universities-to-build-ai-trust-tech-companies-and-non-governmental-organizations-more-than-the-government-to-manage-the-technology" class="section level2">
<h2>Americans place the most trust in the U.S. military and universities to build AI; trust tech companies and non-governmental organizations more than the government to manage the technology</h2>
<!--
BZ: I added more literature review. 

I think this subsection is about 90% there. I think perhaps we need to work on the framing a bit. One thing that could be teased out is that Americans want AI to be regulated but doesn't trust anyone to do it. 
-->
<p>Respondents were asked how much confidence they have in various actors <a href="apptopline.html#trustdevai">to develop AI</a>. They were randomly assigned five actors out of 15 to evaluate. We provided a short description of actors that are not well-known to the public (e.g., NATO, CERN, and OpenAI).</p>
<p>In addition, respondents were asked how much confidence, if any, they have in various actors <a href="apptopline.html#trustmanageai">to manage the development and use of AI</a> in the best interests of the public. They were randomly assigned five out of 15 actors to evaluate. Again, we provided a short description of actors that are not well-known to the public (e.g., AAAI and Partnership on AI). Confidence was measured using the same four-point scale described above.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
<p>In our survey, Americans do not express great confidence in most actors to develop or to manage AI. A majority of Americans do not have a “great deal” or even a “fair amount” of confidence in any institution, except university researchers, to develop AI. Furthermore, Americans place greater trust in tech companies and non-governmental organizations (e.g., OpenAI), compared with governments, to manage the development and use of the technology.</p>
<!-- AD: did you want to produce one of those figures where you can off each proportion? To calculate the above percentages I looked at the appendix tables. I agree with you that it seems better to report meaningful statistics like the proportion who give "a fair amount" or "great deal" of confidence, then the average which is a squishy quasi-meaningless number. 
[BZ: I made the graphs.]

I suggest for the following paragraphs we then give the percentages that give fair amount or great deal for each. 

I don't see why you say openai is that high, it's just a bit above google, microsoft, and tech companies. See exec summary above for my revised interpretation here. I can and will revise this section.
[BZ: I rewrote the interpretation below to reflect your comments.]
-->
<p>University researchers and the U.S. military are the most trusted groups to develop AI: about half of Americans express a “great deal” or even a “fair amount” of confidence in university researchers and the U.S. military. Americans express slightly less confidence in tech companies, non-profit organizations (e.g., OpenAI), and American intelligence organizations. Nevertheless, opinions toward individual actors within each of these groups vary. For example, while 44% of Americans indicated they feel a “great deal” or even a “fair amount” of confidence in tech companies, they rate Facebook as the least trustworthy of all the actors. More than four in 10 indicate that they have no confidence in the company.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<p>The results on the public’s trust of various actors to manage the develop and use of AI provided are similar to the results discussed above. Again, a majority of Americans do not have a “great deal” or even a “fair amount” of confidence in any institution to manage AI. In general, the public expresses greater confidence in non-governmental organizations than in governmental ones. Indeed, 41% of Americans express a “great deal” or even a “fair amount” of confidence in “tech companies,” compared with 26% who felt that way about the U.S. federal government. But when presented with individual big tech companies, Americans indicate less trust in each than in “tech companies.” Once again, Facebook stands out as an outlier: respondents give it a much lower rating than any other actor. Besides “tech companies,” the public places relatively high trust in intergovernmental research organizations (e.g., CERN), the Partnership on AI, and non-governmental scientific organizations (e.g., AAAI). Nevertheless, because the public are less familiar with these organizations, about one in five respondents give a “don’t know” response.</p>
<p>Mirroring our findings, recent survey research suggests that while Americans feel that AI should be regulated, they are unsure <em>who</em> the regulators should be. When asked who “should decide how AI systems are designed and deployed,” half of Americans indicated they do not know or refused to answer <span class="citation">(West <a href="#ref-west2018divided">2018</a><a href="#ref-west2018divided">a</a>)</span>. Our survey results seem to reflect Americans’ general attitudes toward public institutions. Americans placed a great deal of confidence in the U.S. military and scientists to act in the best interest of the public; in contrast, public confidence in elected officials is much lower <span class="citation">(Funk <a href="#ref-funk2017">2017</a>)</span>. Less than one-third of Americans thought that tech companies do what’s right “most of the time” or “just about always”; moreover, more than half think that tech companies have too much power and influence in the U.S. economy <span class="citation">(Smith <a href="#ref-smith2018">2018</a>)</span>. Nevertheless, Americans’ attitude toward tech companies is not monolithic but varies by company. For instance, in a 2018 survey, a higher percentage of Americans trust Apple, Google, Amazon, Microsoft, and Yahoo to protect user information than those who trust Facebook to do so – in line with our research findings <span class="citation">(Ipsos and Reuters <a href="#ref-ipsosreuters2018">2018</a>)</span>.</p>
<div class="figure"><span id="fig:trustdevtable1"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/trustdevtable1-1.png" alt="Trust in various actors to develop AI: distribution of responses" width="2100" />
<p class="caption">
Figure 13: Trust in various actors to develop AI: distribution of responses
</p>
</div>
<div class="figure"><span id="fig:trustdevtable2"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/trustdevtable2-1.png" alt="Trust in various actors to manage AI: distribution of responses" width="2100" />
<p class="caption">
Figure 14: Trust in various actors to manage AI: distribution of responses
</p>
</div>
<div class="figure"><span id="fig:trustcoef"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/trustcoef-1.png" alt="Trust in various actors to develop and manage AI in the interest of the public" width="1800" />
<p class="caption">
Figure 15: Trust in various actors to develop and manage AI in the interest of the public
</p>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-west2018divided">
<p>West, Darrell M. 2018a. “Brookings Survey Finds Divided Views on Artificial Intelligence for Warfare, but Support Rises If Adversaries Are Developing It.” Survey report. Brookings Institution. <a href="https://perma.cc/3NJV-5GV4" class="uri">https://perma.cc/3NJV-5GV4</a>.</p>
</div>
<div id="ref-funk2017">
<p>Funk, Cary. 2017. “Real Numbers: Mixed Messages About Public Trust in Science.” <em>Issues in Science and Technology</em> 34 (1). <a href="https://perma.cc/UF9P-WSRL" class="uri">https://perma.cc/UF9P-WSRL</a>.</p>
</div>
<div id="ref-smith2018">
<p>Smith, Aaron. 2018. “Public Attitudes Toward Technology Companies.” Survey report. Pew Research Center. <a href="https://perma.cc/MKU6-SRLL" class="uri">https://perma.cc/MKU6-SRLL</a>.</p>
</div>
<div id="ref-ipsosreuters2018">
<p>Ipsos and Reuters. 2018. “Facebook Poll 3.23.18.” Survey report. Ipsos; Reuters. <a href="https://perma.cc/W65E-LGWL" class="uri">https://perma.cc/W65E-LGWL</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>According to our regression analysis predicting perceived issue importance by using numeric age, governance challenge, and the interaction between the two, each decade age is correlated with a 0.11 (MOE = 0.01, two-sided <span class="math inline">\(p\)</span>-value &lt; 0.001) point increase, on a four-point scale, in perceived issue importance.<a href="public-opinion-on-ai-governance.html#fnref7">↩</a></p></li>
<li id="fn8"><p>The two sets of 15 actors differed slightly because for some actors it seemed inappropriate to ask one or the other question.<a href="public-opinion-on-ai-governance.html#fnref8">↩</a></p></li>
<li id="fn9"><p>Our survey was conducted three months after the fallout of the Facebook/Cambridge Analytica scandal. Nevertheless, Americans’ distrust of the company existed before the Facebook/Cambridge Analytica scandal. In a pilot survey we conducted on Mechanical Turk during July 13-14, 2017, respondents indicated a substantially lower level of confidence in Facebook, compared with other actors, to develop and regulate AI.<a href="public-opinion-on-ai-governance.html#fnref9">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="general-attitudes-toward-ai.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ai-policy-and-u-s-china-relations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": [["https://www.fhi.ox.ac.uk/wp-content/uploads/Deciphering_Chinas_AI-Dream-1.pdf", "PDF"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
