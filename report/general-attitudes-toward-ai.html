<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Artificial Intelligence: American Attitudes and Trends</title>
  <meta name="description" content="Artificial Intelligence: American Attitudes and Trends">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Artificial Intelligence: American Attitudes and Trends" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Artificial Intelligence: American Attitudes and Trends" />
  
  
  

<meta name="author" content="Baobao Zhang and Allan Dafoe">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="overview-ai-and-american-public-opinion.html">
<link rel="next" href="public-opinion-on-ai-governance.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href=""><b>Contents</b></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="overview-ai-and-american-public-opinion.html"><a href="overview-ai-and-american-public-opinion.html"><i class="fa fa-check"></i>1 Overview: AI and American public opinion</a><ul>
<li class="chapter" data-level="" data-path="overview-ai-and-american-public-opinion.html"><a href="overview-ai-and-american-public-opinion.html#key-findings"><i class="fa fa-check"></i>1.1 Key findings</a></li>
<li class="chapter" data-level="" data-path="overview-ai-and-american-public-opinion.html"><a href="overview-ai-and-american-public-opinion.html#reading-notes"><i class="fa fa-check"></i>1.2 Reading notes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html"><i class="fa fa-check"></i>2 General attitudes toward AI</a><ul>
<li class="chapter" data-level="" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#more-americans-support-than-oppose-developing-ai"><i class="fa fa-check"></i>2.1 More Americans support than oppose developing AI</a></li>
<li class="chapter" data-level="" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#gender-education-income-and-experience-with-tech-predict-support-for-developing-ai"><i class="fa fa-check"></i>2.2 Gender, education, income, and experience with tech predict support for developing AI</a></li>
<li class="chapter" data-level="" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#an-overwhelming-majority-of-americans-think-that-ai-and-robots-should-be-carefully-managed"><i class="fa fa-check"></i>2.3 An overwhelming majority of Americans think that AI and robots should be carefully managed</a></li>
<li class="chapter" data-level="" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#harmful-consequences-of-ai-in-context-of-other-global-risks"><i class="fa fa-check"></i>2.4 Harmful consequences of AI in context of other global risks</a></li>
<li class="chapter" data-level="" data-path="general-attitudes-toward-ai.html"><a href="general-attitudes-toward-ai.html#americans-understanding-of-key-technology-terms"><i class="fa fa-check"></i>2.5 Americans’ understanding of key technology terms</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="public-opinion-on-ai-governance.html"><a href="public-opinion-on-ai-governance.html"><i class="fa fa-check"></i>3 Public opinion on AI governance</a><ul>
<li class="chapter" data-level="" data-path="public-opinion-on-ai-governance.html"><a href="public-opinion-on-ai-governance.html#among-ai-governance-challenges-americans-prioritize-data-privacy-and-preventing-ai-enhanced-cyber-attacks-surveillance-and-digital-manipulation."><i class="fa fa-check"></i>3.1 Among AI governance challenges, Americans prioritize data privacy and preventing AI-enhanced cyber attacks, surveillance, and digital manipulation.</a></li>
<li class="chapter" data-level="" data-path="public-opinion-on-ai-governance.html"><a href="public-opinion-on-ai-governance.html#less-concern-about-ai-governance-challenges-is-expressed-by-americans-who-are-younger-who-have-cs-or-engineering-degress"><i class="fa fa-check"></i>3.2 Less concern about AI governance challenges is expressed by Americans who are younger, who have CS or engineering degress</a></li>
<li class="chapter" data-level="" data-path="public-opinion-on-ai-governance.html"><a href="public-opinion-on-ai-governance.html#americans-place-the-most-trust-in-the-u.s.-military-and-universities-to-build-ai-trust-tech-companies-and-non-governmental-organizations-more-than-the-government-to-manage-the-technology"><i class="fa fa-check"></i>3.3 Americans place the most trust in the U.S. military and universities to build AI; trust tech companies and non-governmental organizations more than the government to manage the technology</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="ai-policy-and-u-s-china-relations.html"><a href="ai-policy-and-u-s-china-relations.html"><i class="fa fa-check"></i>4 AI policy and U.S.-China relations</a><ul>
<li class="chapter" data-level="" data-path="ai-policy-and-u-s-china-relations.html"><a href="ai-policy-and-u-s-china-relations.html#americans-underestimate-the-u.s.-and-chinas-ai-research-and-development"><i class="fa fa-check"></i>4.1 Americans underestimate the U.S. and China’s AI research and development</a></li>
<li class="chapter" data-level="" data-path="ai-policy-and-u-s-china-relations.html"><a href="ai-policy-and-u-s-china-relations.html#communicating-the-dangers-of-a-u.s.-china-arms-race-requires-explaining-policy-trade-offs"><i class="fa fa-check"></i>4.2 Communicating the dangers of a U.S.-China arms race requires explaining policy trade-offs</a></li>
<li class="chapter" data-level="" data-path="ai-policy-and-u-s-china-relations.html"><a href="ai-policy-and-u-s-china-relations.html#americans-see-the-potential-for-u.s.-china-cooperation-on-some-ai-governance-challenges"><i class="fa fa-check"></i>4.3 Americans see the potential for U.S.-China cooperation on some AI governance challenges</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="trend-across-time-attitudes-toward-workplace-automation.html"><a href="trend-across-time-attitudes-toward-workplace-automation.html"><i class="fa fa-check"></i>5 Trend across time: attitudes toward workplace automation</a><ul>
<li class="chapter" data-level="" data-path="trend-across-time-attitudes-toward-workplace-automation.html"><a href="trend-across-time-attitudes-toward-workplace-automation.html#americans-do-not-think-that-labor-market-disruptions-will-increase-with-time"><i class="fa fa-check"></i>5.1 Americans do not think that labor market disruptions will increase with time</a></li>
<li class="chapter" data-level="" data-path="trend-across-time-attitudes-toward-workplace-automation.html"><a href="trend-across-time-attitudes-toward-workplace-automation.html#extending-the-historical-time-trend"><i class="fa fa-check"></i>5.2 Extending the historical time trend</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html"><i class="fa fa-check"></i>6 High-level machine intelligence</a><ul>
<li class="chapter" data-level="" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html#arrivesooner"><i class="fa fa-check"></i>6.1 The public think high-level machine intelligence will arrive sooner than AI experts predict</a></li>
<li class="chapter" data-level="" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html#americans-express-mixed-support-for-developing-high-level-machine-intelligence"><i class="fa fa-check"></i>6.2 Americans express mixed support for developing high-level machine intelligence</a></li>
<li class="chapter" data-level="" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html#men-partisans-high-income-americans-and-those-with-tech-experience-express-greater-support-for-high-level-machine-intelligence"><i class="fa fa-check"></i>6.3 Men, partisans, high-income Americans, and those with tech experience express greater support for high-level machine intelligence</a></li>
<li class="chapter" data-level="" data-path="high-level-machine-intelligence.html"><a href="high-level-machine-intelligence.html#the-public-expects-high-level-machine-intelligence-to-be-more-harmful-than-good"><i class="fa fa-check"></i>6.4 The public expects high-level machine intelligence to be more harmful than good</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appmethod.html"><a href="appmethod.html"><i class="fa fa-check"></i>Appendix A: Methodology</a><ul>
<li class="chapter" data-level="" data-path="appmethod.html"><a href="appmethod.html#yougovsampling"><i class="fa fa-check"></i>YouGov sampling and weights</a></li>
<li class="chapter" data-level="" data-path="appmethod.html"><a href="appmethod.html#appdemosubgroups"><i class="fa fa-check"></i>Demographic subgroups</a></li>
<li class="chapter" data-level="" data-path="appmethod.html"><a href="appmethod.html#appanalysis"><i class="fa fa-check"></i>Analysis</a></li>
<li class="chapter" data-level="" data-path="appmethod.html"><a href="appmethod.html#data-sharing"><i class="fa fa-check"></i>Data sharing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html"><i class="fa fa-check"></i>Appendix B: Topline questionnaire</a><ul>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#global_risks"><i class="fa fa-check"></i>Global risks</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#considersai"><i class="fa fa-check"></i>Survey experiment: what the public considers AI, automation, machine learning, and robotics</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#knowledge-of-computer-science-cstechnology"><i class="fa fa-check"></i>Knowledge of computer science (CS)/technology</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#supportdevai"><i class="fa fa-check"></i>Support for developing AI</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#manageexp"><i class="fa fa-check"></i>Survey experiment: AI and/or robots should be carefully managed</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#trustdevai"><i class="fa fa-check"></i>Trust of actors to develop AI</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#trustmanageai"><i class="fa fa-check"></i>Trust of actors to manage AI</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#govchallenges"><i class="fa fa-check"></i>AI governance challenges</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#airesearchcompare"><i class="fa fa-check"></i>Survey experiment: comparing perceptions of U.S. vs. China AI research and development</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#armsraceexp"><i class="fa fa-check"></i>Survey experiment: U.S.-China arms race</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#control"><i class="fa fa-check"></i>Control</a><ul>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#nationalism-treatment"><i class="fa fa-check"></i>Nationalism treatment</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#war-risks-treatment"><i class="fa fa-check"></i>War risks treatment</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#common-humanity-treatment"><i class="fa fa-check"></i>Common humanity treatment</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#uschinacoop"><i class="fa fa-check"></i>Issue areas for possible U.S.-China cooperation</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#jobtime"><i class="fa fa-check"></i>Trend across time: job creation or job loss</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#forecasthlmi"><i class="fa fa-check"></i>High-level machine intelligence: forecasting timeline</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#supporthlmi"><i class="fa fa-check"></i>Support for developing high-level machine intelligence</a></li>
<li class="chapter" data-level="" data-path="apptopline.html"><a href="apptopline.html#expectedoutcome"><i class="fa fa-check"></i>Expected outcome of high-level machine intelligence</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html"><i class="fa fa-check"></i>Appendix C: Additional data analysis results</a><ul>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#aawhatsai"><i class="fa fa-check"></i>Survey experiment: what the public considers AI, automation, machine learning, and robotics</a><ul>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#addcarefullym"><i class="fa fa-check"></i>Survey experiment: AI and/or robots should be carefully managed</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#appgovchallenges"><i class="fa fa-check"></i>AI governance challenges: prioritizing governance challenges</a></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#trust-in-various-actors-to-develop-and-manage-ai-in-the-interest-of-the-public"><i class="fa fa-check"></i>Trust in various actors to develop and manage AI in the interest of the public</a></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#appuschinacomp"><i class="fa fa-check"></i>Survey experiment: comparing perceptions of U.S. vs. China AI research and development</a></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#survey-experiment-u.s.-china-arms-race"><i class="fa fa-check"></i>Survey experiment: U.S.-China arms race</a></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#appjobloss"><i class="fa fa-check"></i>Trend across time: job creation or job loss</a></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#appsupporthlmi"><i class="fa fa-check"></i>Support for developing high-level machine intelligence</a></li>
<li class="chapter" data-level="" data-path="addresults.html"><a href="addresults.html#expected-outcome-of-high-level-machine-intelligence"><i class="fa fa-check"></i>Expected outcome of high-level machine intelligence</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a><ul>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#primary-researchers"><i class="fa fa-check"></i>Primary researchers</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#editing-and-design"><i class="fa fa-check"></i>Editing and design</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#funders"><i class="fa fa-check"></i>Funders</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#for-media-or-other-inquiries"><i class="fa fa-check"></i>For media or other inquiries</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html#recommended-citation"><i class="fa fa-check"></i>Recommended citation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html"><i class="fa fa-check"></i>About us</a><ul>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#about-the-governance-of-ai-program"><i class="fa fa-check"></i>About the Governance of AI Program</a></li>
<li class="chapter" data-level="" data-path="about-us.html"><a href="about-us.html#about-the-future-of-humanity-institute"><i class="fa fa-check"></i>About the Future of Humanity Institute</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://governance.ai">Governance of AI Program</a></li>
<li><a href="https://www.fhi.ox.ac.uk/">Future of Humanity Institute</a></li>
<li><a href="http://www.ox.ac.uk/">University of Oxford</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Artificial Intelligence: American Attitudes and Trends</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="general-attitudes-toward-ai" class="section level1">
<h1>2 General attitudes toward AI</h1>
<div id="more-americans-support-than-oppose-developing-ai" class="section level2">
<h2>2.1 More Americans support than oppose developing AI</h2>
<p>We measured respondents’ support for the further development of AI after providing them with basic information about the technology. Respondents were given the following definition of AI:</p>
<blockquote>
<p>Artificial Intelligence (AI) refers to computer systems that perform tasks or makes decisions that usually require human intelligence. AI can perform these tasks or make these decisions without explicit human instructions. Today, AI has been used in the following applications: [five randomly selected applications]</p>
</blockquote>
<p>Each respondent viewed five applications randomly selected from <a href="apptopline.html#supportdevai">a list of 14</a> that included translation, image classification, and disease diagnosis. Afterward, respondents were asked <a href="apptopline.html#supportdevai">how much they support or oppose</a> the development of AI.</p>
<div class="figure"><span id="fig:supportdevrisks"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/supportdevrisks-1.png" alt="Support for developing AI" width="2100" />
<p class="caption">
Figure 1: Support for developing AI
</p>
</div>
<p>Americans express mixed support for the development of AI, although more support than oppose the development of AI. A substantial minority (41%) somewhat or strongly support the development of AI. A smaller minority (22%) somewhat or strongly oppose the development of AI. Many express a neutral attitude: 28% state that they neither support nor oppose while 10% indicate they don’t know.</p>
<p>Our survey results reflect the cautious optimism that Americans express in other polls. In a recent survey, 51% of Americans indicated that they support continuing AI research while 31% opposed <span class="citation">(Morning Consult <a href="#ref-morningconsult2017">2017</a>)</span>. Furthermore, 77% of Americans expressed that AI would have a “very positive” or “mostly positive” impact on how people work and live in the next 10 years while 23% thought that AI’s impact would be “very negative” or “mostly negative” <span class="citation">(Northeastern University and Gallup <a href="#ref-negallup2018">2018</a>)</span>.</p>
</div>
<div id="gender-education-income-and-experience-with-tech-predict-support-for-developing-ai" class="section level2">
<h2>2.2 Gender, education, income, and experience with tech predict support for developing AI</h2>
<!--
BZ: I added levels fo education into the regression model and it changed results a bit. I did some literature review looking into how education is correlated with support for AI.

I recommend that you approve the framing of the results. I framed it as people who are more economcially vunerable are less supportive of AI. Let me know if we are overstepping this interpretation. 

AD: Yes, i'm concerned about this, as this is not precisely what we asked about. We should note to ourselves for future studies to ask directly about this. We could say this is plausibly explained by ...
[BZ: I have changed the framing to be more netural.]

AD: why not report gender result? Seems like a strong result. 
[BZ: I changed the title of the subsection to include gender.]

AD: is it weird to have the intercept listed in the figure, as it is basically 0 (presumably it's supposed to be 0?). Also why all the "MOE", seems cluttered. I suggest removing. 
[BZ: I will remove it.]

My assessment: It's about 90% there.
-->
<p>We examined support for developing AI by <a href="appmethod.html#appdemosubgroups">11 demographic subgroup variables</a>, that include age, gender, race, and education. We also used a multiple regression model to predict support for developing AI using these demographic variables.</p>
<p>Support for developing AI varies greatly between demographic subgroups, with gender, education, income, and experience being key predictors. A majority of respondents in each of these four subgroups express support for developing AI: those with four-year college degrees (57%), those with annual household income above $100,000 (59%), those who have completed a computer science or engineering degree (56%), and those with computer science or programming experience (58%). In contrast, women, those with low levels of education, and low-income Americans, are much less enthusiastic about developing AI. One possible explanation for these results is that subgroups that are more vulnerable to workplace automation express less enthusiasm for AI. Within developed countries, women, those with low levels of education, and low-income workers have jobs that are at higher risk of automation, according to an analysis by the Organisation for Economic Co-operation and Development <span class="citation">(Nedelkoska and Quintini <a href="#ref-nedelkoska2018automation">2018</a>)</span>.</p>
<p>We used a multiple regression that includes all of the demographic variables to predict support for developing AI. The support for developing AI outcome variable was standardized, such that it has mean 0 and unit variance.</p>
<p>Significant predictors correlated with <em>support</em> for developing AI include:</p>
<ul>
<li>Identifying as a Democrat (versus identifying as an Independent)</li>
<li>Having graduated from a four-year college (versus having a high school degree or less)</li>
<li>Having a family income of greater than $100,000 annually (versus having a family income of less than $30,000 annually)</li>
<li>Having CS or programming experience (versus not having such experience)</li>
</ul>
<p>Significant predictors correlated with <em>opposition</em> to developing AI include:</p>
<ul>
<li>Being a Gen Xer or Baby Boomer (versus being a Millennial/post-Millennial)</li>
<li>Being a female (versus being a male)</li>
<li>Preferring not to say one’s annual family income (versus having a family income of less than $30,000 annually) <!-- Um, this leads me to wonder whether vs other comparisons is not significant. But actually other comparisons are more significant. I think better to say this, eg versus reporting any other level of income. 
[BZ: Can you explain your comment? I am not understanding it. The category that was left out was having a family income of less than $30K annually -- that's the baseline group. Are you suggesting that we use "not reporting one's income" as the baseline group instead? I think it's more work to fix it now.]
--></li>
<li>Identifying as a Christian</li>
</ul>
<p>Some of the demographic differences we observe in this survey are in line with existing public opinion research. Below we highlight three salient predictors of support for AI based on the existing literature: gender, education, and income.</p>
<p>Around the world, women view AI more negatively than men. Fifty-four percent of women in EU countries view AI positively, compared with 67% of men <span class="citation">(Eurobarometer <a href="#ref-eurobarometer460">2017</a>)</span>. Likewise in the U.S., 44% of women perceive AI as unsafe – compared with 30% of men <span class="citation">(Morning Consult <a href="#ref-morningconsult2017">2017</a>)</span>. This gender difference could be explained by the fact that women express higher distrust of technology than men do. In the U.S., women – compared with men – are more likely to view genetically modified foods or foods treated with pesticides as unsafe to eat, to oppose building more nuclear power plants, and to oppose fracking <span class="citation">(Funk and Rainie <a href="#ref-funk2015">2015</a>)</span>.</p>
<p>One’s level of education also predicts one’s enthusiasm toward AI, according to existing research. Reflecting upon their own jobs, 32% of Americans with no college degree thought that technology had increased their opportunities to advance – compared with 53% of Americans with a college degree <span class="citation">(Smith and Anderson <a href="#ref-smith2017">2016</a>)</span>. Reflecting on the economy at large, 38% of those with post-graduate education felt that automation had helped American workers while only 19% of those without a college degree thought so <span class="citation">(Graham <a href="#ref-graham2018">2018</a>)</span>. Likewise in the EU, those with more years of education, relative to those with fewer years, were more likely to value AI as good for society and less likely to think that AI steals people’s jobs.</p>
<p>Another significant demographic divide in attitudes toward AI is income: low-income respondents, compared with high-income respondents, view AI more negatively. For instance, 40% of EU residents who had difficulty paying their bills “most of the time” hold negative views toward robots and AI, compared with 27% of those who “almost never” or “never” had difficulty paying their bills <span class="citation">(Eurobarometer <a href="#ref-eurobarometer460">2017</a>)</span>. In the U.S., 16% of those who made less than $50,000 annually think that they are likely to lose their job to automation – compared with only 8% of Americans who made greater than $100,000 annually <span class="citation">(Graham <a href="#ref-graham2018">2018</a>)</span>. Furthermore, Americans’ belief that AI will help the economy or support for AI research is positively correlated with their income <span class="citation">(Morning Consult <a href="#ref-morningconsult2017">2017</a>)</span>.</p>
<div class="figure"><span id="fig:demographicsupportstackbar"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/demographicsupportstackbar-1.png" alt="Predicting support for developing AI using demographic characteristics: distribution of responses" width="2550" />
<p class="caption">
Figure 2: Predicting support for developing AI using demographic characteristics: distribution of responses
</p>
</div>
<div class="figure"><span id="fig:demographicsupportregression"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/demographicsupportregression-1.png" alt="Predicting support for developing AI using demographic characteristics: average support across groups" width="2100" />
<p class="caption">
Figure 3: Predicting support for developing AI using demographic characteristics: average support across groups
</p>
</div>
<div class="figure"><span id="fig:demographicsupport2"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/demographicsupport2-1.png" alt="Predicting support for developing AI using demographic characteristics: results from a multiple regression model that includes all demographic variables" width="2100" />
<p class="caption">
Figure 4: Predicting support for developing AI using demographic characteristics: results from a multiple regression model that includes all demographic variables
</p>
</div>
</div>
<div id="an-overwhelming-majority-of-americans-think-that-ai-and-robots-should-be-carefully-managed" class="section level2">
<h2>2.3 An overwhelming majority of Americans think that AI and robots should be carefully managed</h2>
<!-- 
BZ: I think this section is pretty solid.
-->
<p>To compare Americans’ attitudes with those of EU residents, we performed a <a href="apptopline.html#manageexp">survey experiment</a> that replicated a question from the 2017 Special Eurobarometer #460. The original question asked respondents how much they agree or disagree with the following statement:</p>
<blockquote>
<p>Robots and artificial intelligence are technologies that require careful management.</p>
</blockquote>
<p>We asked a similar question except respondents were randomly assigned to consider one of these three statements:</p>
<ul>
<li>AI and robots are technologies that require careful management.</li>
<li>AI is a technology that requires careful management.</li>
<li>Robots are technologies that require careful management.</li>
</ul>
<p>Our respondents were given the <a href="apptopline.html#manageexp">same answer choices</a> presented to the Eurobarometer subjects.</p>
<p>The overwhelming majority of Americans – more than eight in 10 – agree that AI and/or robots should be carefully managed, while only 6% disagree.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> We find that variations in the statement wording produce <a href="addresults.html#addcarefullym">minor, non-significant, differences</a> in responses.</p>
<div class="figure"><span id="fig:aimanaged"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/aimanaged-1.png" alt="Agreement with statement that AI and/or robots should be carefully managed" width="672" />
<p class="caption">
Figure 5: Agreement with statement that AI and/or robots should be carefully managed
</p>
</div>
<div class="figure"><span id="fig:aimanagedexp2"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/aimanagedexp2-1.png" alt="Agreement with statement that AI and/or robots should be carefully managed by experimental condition" width="2100" />
<p class="caption">
Figure 6: Agreement with statement that AI and/or robots should be carefully managed by experimental condition
</p>
</div>
<p>Next, we compared our survey results with the ones from the 2017 Special Eurobarometer #460 by country <span class="citation">(Eurobarometer <a href="#ref-eurobarometer460">2017</a>)</span>. For the U.S., we used all the responses in our survey, unconditional on the experimental condition given that the variation in question-wording does not appear to affect responses.</p>
<p>The percentage of those in the U.S. who agree with the statement (82%) is not far off from the EU average (88% agree with the statement). Likewise, the percentage of Americans who disagree with the statement (6% disagree) is comparable with the EU average (7% disagree). The U.S. ranks among the lowest in terms of the agreement with the statement in part due to the relatively high percentage who selects the “don’t know” option.</p>
<div class="figure"><span id="fig:eu"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/eu-1.png" alt="Agreement with statement that robots and AI require careful management (EU data from 2017 Special Eurobarometer #460)" width="2100" />
<p class="caption">
Figure 7: Agreement with statement that robots and AI require careful management (EU data from 2017 Special Eurobarometer #460)
</p>
</div>
</div>
<div id="harmful-consequences-of-ai-in-context-of-other-global-risks" class="section level2">
<h2>2.4 Harmful consequences of AI in context of other global risks</h2>
<!-- 
BZ: I moved this subsection here instead of at the end of the report because I think the results are important enough to not bury it. I added the comparison with the WEF expert survey results. I left some comments below regarding the interpretation of the AI results; I think your editorial recommendations might be over-interpreting the results. 

Another point: Do you think we should add some citations that explains why some experts think AI is a global risk?

My assessment is that it's about 85% there.

AD: should we page break before getting into this? It seems weird to have the headline come right after "agree with statement" on careful management figure. 
[BZ: This problem will likely go away as we edit and move content around.]
-->
<p>At the beginning of the survey, respondents were asked to consider five out of <a href="apptopline.html#global_risks">15 potential global risks</a>. The purpose of this task is to compare respondents’ perception of AI as a global risk compared with other potential risks. The global risks are selected from the World Economic Forum’s <a href="https://www.weforum.org/reports/the-global-risks-report-2018">“The Global Risk Report 2018”</a>. We simplified the language used to describe each risk so that the average reader can understand it, while preserving the substantive content. We gave the following definition for a global risk:</p>
<blockquote>
<p>A “global risk” is an uncertain event or condition that, if it happens, could cause significant negative impact for at least 10 percent of the world’s population. That is, at least 1 in 10 people around the world could experience a significant negative impact.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
</blockquote>
<!-- AD: you just said that. remove one of them
[BZ: The two quotes are different. I moved the second one into a footnote.] -->
<p>After considering each potential global risk, respondents were asked to evaluate the likelihood of it happening globally within 10 years and its impact on several countries or industries.</p>
<p>We use a scatterplot to visualize results from respondents’ evaluations of global risks. The x-axis is the perceived likelihood of the risk happening globally within 10 years. The y-axis is the perceived impact of the risk. The mean perceived likelihood and impact is represented by a dot. The corresponding ellipse contains the 95% confidence region.</p>
<p>In general, Americans perceive all these risks to be impactful: on average they rate each as having between a moderate (2) and severe (3) negative impact if they were to occur. Americans perceive the use of weapons of mass destruction to be the most impactful – at the “severe” level (mean score 3.0 out of 4). Although they do not think this risk as likely as other risks, they still assign it an average of 48% probability of occurring within 10 years. Risks in the upper-right quadrant are perceived to be the most likely as well as the most impactful. These include natural disasters, cyber attacks, and extreme weather events.</p>
<p>The American public and the nearly 1,000 experts surveyed by the World Economic Forum share similar views regarding most of the potential global risks <span class="citation">(World Economic Forum <a href="#ref-wef2018">2018</a>)</span>. Both the public and the experts rank extreme weather events, natural disasters, and cyber attacks as the top three most likely global risks; likewise, both groups consider weapons of mass destruction to be the most impactful. Nevertheless, compared with experts, Americans offer a lower estimate of the likelihood and impact of the failure to address climate change.</p>
<p>The reported likelihoods of these risks appear to be over-estimates. The mean responses suggest (assuming independence) that about eight (out of 15) of these global risks, which will have a significant negative impact on at least 10% of the world’s population, will take place in the next 10 years. One explanation for this is that it arises from the broad misconception that the world is in a much worse state than it is in reality <span class="citation">(Pinker <a href="#ref-pinker2018enlightenment">2018</a>; Rosling, Rönnlund, and Rosling <a href="#ref-rosling2018factfulness">2018</a>)</span>. Another explanation is that it arises as a byproduct of respondents interpreting “significant negative impact” in a relatively minimal way, though this interpretation is hard to sustain given the mean severity being between “moderate” and “severe.” Finally, this result may be because subjects centered their responses within the distribution of our response options, the middle value of which was the 40-60% option; thus, the likelihoods should not be interpreted literally in absolute sense.</p>
<p>The adverse consequences of AI within the next 10 years appear to be a relatively low priority in respondents’ assessment of global risks. It – along with adverse consequences of synthetic biology – occupy the lower left quadrant, which contains what are perceived to be lower-probability, lower-impact risks.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> These risks are perceived to be as impactful (within the next 10 years) as the failure to address climate change, though less probable. One interpretation of this is that the average American simply does not regard AI (or synthetic biology) as posing substantial global risks. This interpretation, however, would be in tension with some expert assessment of catastrophic risks that suggests unsafe AI could pose significant danger <span class="citation">(World Economic Forum <a href="#ref-wef2018">2018</a>; Sandberg and Bostrom <a href="#ref-sandberg2008">2008</a>)</span>. The gap between experts and the public’s assessment suggests that this is a fruitful area for efforts to educate the public.</p>
<!-- AD: Optional text, prob delete
This interpretation, however, would be in tension with some expert assessment of catastrophic risks, and suggests that future dialogue between experts and the public would be productive.   AD: maybe cite open phil, Global catastrophic risks survey, and maybe WEF?
[BZ: I added a sentence above.]
[MA: Made a bit more clear that this provides an opportunity to educate the public]
-->
<p>Another interpretation is that Americans do have substantial concerns about the long-run impacts of advanced AI, but they do not see these risks as likely in the coming 10 years. As support for this interpretation, we later find that 12% of American’s believe the impact of high-level machine intelligence will be “extremely bad, possibly human extinction”, and 22% that it will be “on balance bad.” However, subjects may believe that the risks from high-level machine intelligence are not likely to manifest in 10 years: the mean response expects around a <a href="high-level-machine-intelligence.html#arrivesooner">54%</a> chance of high-level machine intelligence within 10 years. If we assume respondents believe global catastrophic risks from AI only emerge from high-level AI, we can infer an implied global risk, conditional on high-level AI (within 10 years), of 80%. Future work should try to unpack and understand these beliefs.</p>
<!-- BZ: I think the following interpretation is interpreting it too much. I propose that we instead cite some literature on experts using that AI should be considered a serious global risk...to focus on educating the public rather than over-interpreting this result too much. 
AD: Hmm, I think my interpretation is reasonable given later HLMI result.  I've extensively revised. Please see above. Also, pelase check probability 55% which I read off of figure on p 35 (should our figures be labeled?)
[BZ: The 54% checks out from the Table. I will add table and figures numbers back.]
-->
<!-- AD: i'm confused. did WEF explicitly call out Ai and synth bio? If not, shouldn't call it out here, as it is misleading. Just say that WEF talks about tech advances. 
[BZ: I made made the language more explicit by including quotations of the survey question.]
-->
<div class="figure"><span id="fig:globalrisksfig"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/globalrisksfig-1.png" alt="The public's perceptions of 15 potential global risks" width="2100" />
<p class="caption">
Figure 8: The public’s perceptions of 15 potential global risks
</p>
</div>
</div>
<div id="americans-understanding-of-key-technology-terms" class="section level2">
<h2>2.5 Americans’ understanding of key technology terms</h2>
<!--
BZ: I added information that explains the non-response is correlated with respondent inattention. I also did F-tests to show that responses are different for the terms within each technological application. The results of the analysis are found in Appendix C.

I'm looking for advice regarding where to put this subsection.
Reasons for putting it here: It makes sense chronologically because it's most of the first things that we asked. It's asked before we define AI for respondents. 

Reason for not putting it here: It's not important enough to be the first subsection. It might make readers think that our sample is low quality (because of inattention) or that the public is ignorant so their opinions don't matter.

We could put it at the end of this section and explain that it was asked very early in the survey, before we defined AI. 

My assessment: It's 95% there -- mainly we need to figure out where to put this subsection. 

AD: why is it written in present tense? started changing it to past tense.
Should we have a concise title for the subsection? Eg “Public Understanding of Key Terms”.
[BZ: I shortened the subsection title as you had suggested.]
Or we should make more clear visually that the heading is a summary of the result. But then we need to make discussion beneath it focused on that.   
Yes, I think this should go later. It is the least interesting. 
[BZ: I moved it to here.]
Should quote terms throughout, or italicize do not use “AI” or “machine learning”.
[BZ: I made everything italics.]

The discussion was a bit confusing, as each paragraph offers an "explanation", but it wasn't completely clear what was being explained. Can you state (or visually call out) more clearly the "finding", which is that AI was under-answered. 
[BZ: I rewrote the discussion to be more explicit.]

Also, what about the other finding, eg that there was some nuance in how these terms were understood? I think that is worth noting, to show how these terms aren't all understood the same way, and the variation roughly corresponds to how experts understand it. But also we shouldn't make too much about this, given the results are modest. 
-->
<p>We used a <a href="apptopline.html#considersai">survey experiment</a> to understand how the public understands the terms <em>AI</em>, <em>automation</em>, <em>machine learning</em>, and <em>robotics</em>. Each respondent was randomly assigned to consider one of these terms; he or she was asked:</p>
<blockquote>
<p>In your opinion, which of the following technologies, if any, uses [artificial intelligence (AI)/automation/machine learning/robotics]? Select all that apply.</p>
</blockquote>
<p>Because we wanted to understand respondents’ perceptions of these terms, we did not define any of the terms. Respondents were asked consider <a href="apptopline.html#considersai">10 technological applications</a>. All of the applications described use AI or machine learning.</p>
<p>Though the respondents show some understanding of the terms, in correctly applying them to some technologies, the respondents underestimate the prevalence of AI, machine learning, and robotics in everyday technological applications. (See <a href="addresults.html#aawhatsai">Appendix C</a> for details of our statistical analysis.)</p>
<!-- MA to BZ: Changed the above around. Should be looked over. I also think that maybe a sentence should be added stating that all of the listed technologies _actually_ use AI/ML. E.g. "Among those assigned the term _AI_, ... and autonomous drones use AI (54%), despite all of these applications using the technology." Another solution would be to state what types of technology is used in the applications in the bullet list above in paranthesis or something. 
-->
<p>Among those assigned the term <em>AI</em>, a majority think that virtual assistants (63%), smart speakers (55%), driverless cars (56%), and autonomous drones use AI (54%). A majority of respondents assume that Facebook photo tagging, Google Search, Netflix or Amazon recommendations, or Google Translate do not use AI.</p>
<p>Why did so few respondents consider the products and services we listed to be applications of AI, automation, machine learning, or robotics?</p>
<p>A straightforward explanation is that inattentive respondents neglect to select the items presented to them (i.e., non-response). Even among those assigned the term <em>robotics</em>, only 62% selected social robots and 70% selected industrial robots. Our <a href="addresults.html#aawhatsai">analysis</a> confirms that respondent inattention, defined as spending too little or too much time on the survey, predicts non-response to this question.</p>
<p>Another potential explanation for the results is that the American public – like the public elsewhere – lack awareness of AI or machine learning. As a result, the public does not know that many tech products and services use AI or machine learning. According to a 2017 survey, nearly half of Americans reported that they are unfamiliar with AI <span class="citation">(Morning Consult <a href="#ref-morningconsult2017">2017</a>)</span>. In the same year, only 9% of the British public said they had heard of the term “machine learning” <span class="citation">(Ipsos MORI <a href="#ref-rs2018">2018</a>)</span>. Similarly, less than half of EU residents reported hearing, reading, or seeing something about AI in the previous year <span class="citation">(Eurobarometer <a href="#ref-eurobarometer460">2017</a>)</span>.</p>
<p>Finally, the so-called “AI effect” could also explain the survey result. The AI effect describes the phenomenon that the public does not consider an application of AI to use AI once that application becomes commonplace <span class="citation">(McCorduck <a href="#ref-McCorduck2004">2004</a>)</span>. Because 85% of Americans report using a digital product that deploys AI (e.g., navigation apps, video or music streaming apps, digital personal assistants on smartphones, etc.) <span class="citation">(Reinhart <a href="#ref-reinhart2018">2018</a>)</span>, they may not think that these everyday applications deploy AI.</p>
<!-- AD: above looks good. Optional analysis: see if any of the specific examples shifts approval around at all. I doubt there is a big effect there, and it would be an underpowered analysis. Should we log such analysis ideas somewhere? Perhaps in an appendix for us? Maybe if we get volunteers at some point we could have them do these optional exploratory analyses? 
[BZ: I will do it if I have more time...right now it's not a priority.]
-->
<div class="figure"><span id="fig:whatai"></span>
<img src="ai_public_opinion_us_2018_report-181121-BZ_web_files/figure-html/whatai-1.png" alt="What applications or products that the public thinks use AI, automation, machine learning, or robotics" width="2100" />
<p class="caption">
Figure 9: What applications or products that the public thinks use AI, automation, machine learning, or robotics
</p>
</div>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-morningconsult2017">
<p>Morning Consult. 2017. “National Tracking Poll 170401.” Survey report. Morning Consult. <a href="https://perma.cc/TBJ9-CB5K" class="uri">https://perma.cc/TBJ9-CB5K</a>.</p>
</div>
<div id="ref-negallup2018">
<p>Northeastern University and Gallup. 2018. “Optimsim and Axiety: Views on the Impact of Artificial Intelligence and Higher Education’s Response.” Survey report. Northeastern University; Gallup. <a href="https://perma.cc/57NW-XCQN" class="uri">https://perma.cc/57NW-XCQN</a>.</p>
</div>
<div id="ref-nedelkoska2018automation">
<p>Nedelkoska, Ljubica, and Glenda Quintini. 2018. “Automation, Skills Use and Training.” Working Papers No. 202. Organisation for Economic Co-operation; Development. <a href="https://doi.org/10.1787/2e2f4eea-en" class="uri">https://doi.org/10.1787/2e2f4eea-en</a>.</p>
</div>
<div id="ref-eurobarometer460">
<p>Eurobarometer. 2017. “Special Eurobarometer 460: Attitudes Towards the Impact of Digitisation and Automation on Daily Life.” Eurobarometer. <a href="https://perma.cc/VP6V-43UC" class="uri">https://perma.cc/VP6V-43UC</a>.</p>
</div>
<div id="ref-funk2015">
<p>Funk, Cary, and Lee Rainie. 2015. “Public and Scientists’ Views on Science and Society.” Survey report. Pew Research Center. <a href="https://perma.cc/WXU9-A7VX" class="uri">https://perma.cc/WXU9-A7VX</a>.</p>
</div>
<div id="ref-smith2017">
<p>Smith, Aaron, and Monica Anderson. 2016. “Automation in Everyday Life.” Pew Research Center. <a href="https://perma.cc/WDX9-7WQH" class="uri">https://perma.cc/WDX9-7WQH</a>.</p>
</div>
<div id="ref-graham2018">
<p>Graham, Edward. 2018. “Views on Automation’s U.S. Workforce Impact Highlight Demographic Divide.” Morning Consult. <a href="https://perma.cc/544D-WRUM" class="uri">https://perma.cc/544D-WRUM</a>.</p>
</div>
<div id="ref-wef2018">
<p>World Economic Forum. 2018. “The Global Risks Report 2018: 13th Edition.” World Economic Forum. <a href="https://perma.cc/8XM8-LKEN" class="uri">https://perma.cc/8XM8-LKEN</a>.</p>
</div>
<div id="ref-pinker2018enlightenment">
<p>Pinker, Steven. 2018. <em>Enlightenment Now: The Case for Reason, Science, Humanism, and Progress</em>. New York: Penguin.</p>
</div>
<div id="ref-rosling2018factfulness">
<p>Rosling, Hans, Anna Rosling Rönnlund, and Ola Rosling. 2018. <em>Factfulness: Ten Reasons We’re Wrong About the World–and Why Things Are Better Than You Think</em>. New York: Flatiron Books.</p>
</div>
<div id="ref-sandberg2008">
<p>Sandberg, Anders, and Nick Bostrom. 2008. “Global Catastrophic Risks Survey.” Technical report #2008-1. Future of Humanity Institute, Oxford University. <a href="https://perma.cc/TA97-KD3Z" class="uri">https://perma.cc/TA97-KD3Z</a>.</p>
</div>
<div id="ref-rs2018">
<p>Ipsos MORI. 2018. “Public Views of Machine Learning: Findings from Public Research and Engagement Conducted on Behalf of the Royal Society.” Survey report. The Royal Society. <a href="https://perma.cc/79FE-TEHH" class="uri">https://perma.cc/79FE-TEHH</a>.</p>
</div>
<div id="ref-McCorduck2004">
<p>McCorduck, Pamela. 2004. <em>Machines Who Think: A Personal Inquiry into the History and Prospects of Artificial Intelligence</em>. New York: A K Peters/CRC Press.</p>
</div>
<div id="ref-reinhart2018">
<p>Reinhart, RJ. 2018. “Most Americans Already Using Artificial Intelligence Products.” Survey report. Gallup. <a href="https://perma.cc/RVY5-WP9W" class="uri">https://perma.cc/RVY5-WP9W</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>These percentages that we discuss here reflect the average response across the three statements. See <a href="apptopline.html#manageexp">Appendix B</a> for the topline result for each statement.<a href="general-attitudes-toward-ai.html#fnref4">↩</a></p></li>
<li id="fn5"><p>Our definition of global risk borrowed from the Global Challenges Foundation’s definition: “an uncertain event or condition that, if it happens, can cause a significant negative impact on at least 10% of the world’s population within the next 10 years” <span class="citation">(Cotton-Barratt et al. <a href="#ref-cotton2016">2016</a>)</span>.<a href="general-attitudes-toward-ai.html#fnref5">↩</a></p></li>
<li id="fn6"><p>The World Economic Forum’s survey asked experts to evaluate the “adverse consequences of technological advances,” defined as “[i]ntended or unintended adverse consequences of technological advances such as artificial intelligence, geo-engineering and synthetic biology causing human, environmental and economic damage.” The experts considered these “adverse consequences of technological advances” to be less likely and lower-impact, compared with other potential risks.<a href="general-attitudes-toward-ai.html#fnref6">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="overview-ai-and-american-public-opinion.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="public-opinion-on-ai-governance.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"download": [["https://www.fhi.ox.ac.uk/wp-content/uploads/Deciphering_Chinas_AI-Dream-1.pdf", "PDF"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
